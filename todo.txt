(B) Add configurable timeout to all LLM generate calls in src/services/llm_service.py: default 120s for generation, 30s for health checks; raise TimeoutError with descriptive message +stability
(B) Move hardcoded corpus path (corpus/clean/tort/corpus.json) in src/services/corpus_service.py to settings.corpus_path; move hardcoded DB path (data/jikai.db) in src/services/database_service.py to settings.database_path; move embedding model name in src/services/vector_service.py to settings +refactor
(B) Add API endpoints for new features in src/api/main.py: POST /ml/train (trigger training), GET /ml/status (model status+metrics), GET /providers (list providers+models), PUT /providers/default (set default), GET /generate with provider+model query params +feature
(B) Create src/tui/widgets/loading.py: reusable loading spinner widget, progress bar widget with percentage+ETA, status checklist widget (green check / red x / yellow spinner per item) for use across all screens +feature
(B) Create src/tui/widgets/topic_selector.py: multi-select checkbox tree grouped by tort category (General Negligence, Intentional Torts, Land Torts, Strict Liability, Defences); select-all per category; search/filter; show count of selected +feature
(B) Update Makefile: remove docker targets (docker-build, docker-up, docker-down), add `tui` target (python -m src.tui), add `api` target (uvicorn), add `train` target (python -m src.ml.pipeline --data corpus/labelled.csv), add `test` target (pytest), add `setup` target (pip install -r requirements.txt) +infra
(B) Add tests for AnthropicProvider, GoogleGeminiProvider, LocalLLMProvider in tests/test_services/test_llm_providers.py: mock HTTP responses, test retry logic, test error handling, test model listing +feature
(B) Add tests for ML pipeline in tests/test_ml/: test_classifier.py (train on small fixture, predict, evaluate metrics), test_regressor.py (train, predict, evaluate), test_clustering.py (fit, predict_cluster), test_pipeline.py (train_all, predict) +feature
(B) Add snapshot tests for TUI screens in tests/test_tui/: test each screen renders correctly with mock data, test navigation between screens, test generate flow with mock LLM response +feature
(B) Create sample labelled training data at corpus/labelled/sample.csv with 20 entries: columns text,topic_labels,quality_score,difficulty_level; topic_labels as pipe-separated string; quality_score 1.0-10.0; difficulty_level easy/medium/hard +feature
(B) Add generation method selection to GenerationRequest model in src/services/hypothetical_service.py: add field method: Literal["pure_llm", "ml_assisted", "hybrid"] with default "pure_llm"; add field provider: Optional[str] and model: Optional[str] for per-request LLM selection +feature
(B) Implement hybrid generation mode in src/services/hypothetical_service.py: use ML classifier to suggest/validate topics, generate with LLM, use ML regressor to score output, if score < threshold regenerate with adjusted prompt, use clusterer to check diversity against last 10 generations +feature
(B) Remove all Docker references from README.md: remove Docker setup instructions, add local setup instructions (python -m venv, pip install, make setup, make tui), add section on API key configuration, add section on ML training +infra
(B) Add error boundary and circuit breaker pattern to LLM provider calls: after 3 consecutive failures on a provider, mark it as unhealthy for 60s, auto-fallback to next available provider, show status in TUI provider screen +stability
(B) Create src/services/llm_providers/__init__.py that auto-discovers and registers all provider classes from the package, exports ProviderRegistry singleton +refactor
(B) Add cost tracking to LLM providers: estimate token costs per provider/model (configurable price table in settings), accumulate per-session cost, display in TUI footer and generation results +feature
(B) Add corpus import/export in TUI corpus screen: import from CSV/JSON file, export selected entries to CSV/JSON, validate schema on import +feature
(B) Update src/services/prompt_engineering/templates.py: make templates aware of expanded subtopics, add topic-specific prompt hints for new subtopics (e.g. occupiers_liability -> include premises description, visitor classification) +feature
(B) Implement async streaming in TUI generate screen: if provider supports streaming, show tokens appearing in real-time in output panel; fallback to loading spinner + full response for non-streaming providers +feature
