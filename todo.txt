(A) Remove Dockerfile, docker-compose.yml, docker-compose.dev.yml, and all Docker-related references from Makefile and README +infra
(A) Update requirements.txt: add textual>=0.47, anthropic>=0.40, google-generativeai>=0.8, rich>=13.0, scikit-learn>=1.4, joblib>=1.3, pandas>=2.2; remove unused alembic, asyncio-mqtt, langchain, nginx, redis references +infra
(A) Create abstract LLM provider interface in src/services/llm_providers/base.py with methods: generate(prompt, system_prompt, temperature, max_tokens) -> LLMResponse, list_models() -> list[str], health_check() -> dict, stream_generate() -> AsyncIterator; include provider registry class with register/get/list_providers methods +feature
(A) Implement AnthropicProvider in src/services/llm_providers/anthropic_provider.py using anthropic SDK: support claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001, claude-opus-4-6; handle API key from settings; implement retry with backoff; map to base interface +feature
(A) Implement GoogleGeminiProvider in src/services/llm_providers/google_provider.py using google-generativeai SDK: support gemini-2.0-flash, gemini-2.5-pro; handle API key from settings; implement retry with backoff; map to base interface +feature
(A) Refactor existing OllamaProvider into src/services/llm_providers/ollama_provider.py: extract from llm_service.py, conform to new base interface, add dynamic model listing via GET /api/tags, keep retry logic +refactor
(A) Refactor existing OpenAIProvider into src/services/llm_providers/openai_provider.py: extract from llm_service.py, conform to new base interface, add dynamic model listing, support gpt-4o/gpt-4o-mini/o1/o3-mini +refactor
(A) Add LocalLLMProvider in src/services/llm_providers/local_provider.py: support llama.cpp server HTTP API at configurable host:port, dynamic model listing, conform to base interface +feature
(A) Rewrite src/services/llm_service.py to use provider registry: initialize all configured providers on startup, expose select_provider(name)/select_model(name) methods, allow per-request provider+model override in generate() +refactor
(A) Create src/ml/data.py: load labelled corpus from CSV (columns: text, topic_labels, quality_score, difficulty_level), train/test split, TF-IDF feature extraction with configurable max_features/ngram_range, label binarization for multi-label topics +feature
(A) Create src/ml/classifier.py: multi-label topic classifier using sklearn OneVsRestClassifier with LinearSVC or LogisticRegression; methods: train(X, y) with progress callback, predict(text) -> list[str], evaluate() -> dict(precision, recall, f1 per topic), save_model(path), load_model(path) using joblib +feature
(A) Create src/ml/regressor.py: quality score regressor using sklearn GradientBoostingRegressor or RandomForestRegressor; methods: train(X, y) with progress callback, predict(text) -> float, evaluate() -> dict(mse, r2, mae), save_model(path), load_model(path) using joblib +feature
(A) Create src/ml/clustering.py: hypothetical clustering using sklearn KMeans and DBSCAN on TF-IDF vectors; methods: fit(corpus_texts, n_clusters) with progress callback, predict_cluster(text) -> int, get_cluster_summary() -> dict, visualize_clusters() -> str (ASCII table of cluster stats), save_model(path), load_model(path) +feature
(A) Create src/ml/pipeline.py: unified ML pipeline orchestrator that coordinates classifier, regressor, clusterer; methods: train_all(data_path, progress_callback), train_single(model_type, data_path, progress_callback), evaluate_all() -> dict, predict(text) -> dict(topics, quality, cluster), get_status() -> dict(trained models, metrics) +feature
(A) Create src/tui/app.py: Textual App subclass as main TUI entry point with header, footer (keybindings), sidebar navigation (Generate, Train ML, Browse Corpus, Settings, Provider Config), content area; dark theme; bind q=quit, g=generate, t=train, s=settings +feature
(A) Create src/tui/screens/generate.py: Textual Screen for hypothetical generation; widgets: topic multi-select checklist (all tort subtopics), provider dropdown, model dropdown (updates dynamically per provider), complexity slider, party count input, method radio (Pure LLM / ML-Assisted / Hybrid), generate button; on submit: show loading spinner, stream output to scrollable panel, show validation results as colored checklist, show analysis in collapsible panel +feature
(A) Create src/tui/screens/train.py: Textual Screen for ML training; widgets: data file selector (file browser), model type checkboxes (classifier/regressor/clusterer), hyperparameter inputs (n_clusters, test_split, max_features), train button; on train: show per-model progress bar with percentage and ETA, show live metrics table (precision/recall/f1/mse/r2), show completion status with green checkmarks, save model button +feature
(A) Create src/tui/screens/corpus.py: Textual Screen for corpus browsing; widgets: searchable DataTable of all hypotheticals (columns: id, topics, quality_score, word_count, preview), topic filter dropdown, sort controls, detail panel showing full text on row select, cluster visualization if trained +feature
(A) Create src/tui/screens/settings.py: Textual Screen for app settings; widgets: form inputs for API keys (Anthropic, OpenAI, Google, masked input), Ollama host URL, local LLM host URL, default temperature slider, default max_tokens input, corpus path selector, database path selector, log level dropdown; save button persists to .env or config.toml +feature
(A) Create src/tui/screens/providers.py: Textual Screen for LLM provider management; widgets: provider list with health status indicators (green/red dots), per-provider model list fetched dynamically, test connection button per provider, set default provider+model controls +feature
(A) Expand tort subtopics in src/config/settings.py: add occupiers_liability, product_liability, contributory_negligence, economic_loss, psychiatric_harm, employers_liability, breach_of_statutory_duty, rylands_v_fletcher, consent_defence, illegality_defence, limitation_periods, res_ipsa_loquitur, novus_actus_interveniens, volenti_non_fit_injuria to default topic list +feature
(A) Add keyword definitions for each new tort subtopic in src/services/validation_service.py following existing pattern: occupiers_liability=[occupier, visitor, premises, invitee, licensee, trespasser], product_liability=[manufacturer, product, defect, consumer, safety], etc. for all new topics +feature
(A) Update src/config/settings.py: add LLMProviderSettings(anthropic_api_key, google_api_key, openai_api_key, ollama_host, local_llm_host, default_provider, default_model), MLSettings(models_dir, training_data_path, default_n_clusters), TUISettings(theme, keybindings) +feature
(A) Create src/tui/__main__.py as CLI entry point: parse args (--api-only, --tui-only, --both), when --both: run FastAPI in background thread + Textual TUI in main thread; add to Makefile as `make tui` and `make api` and `make run` (both) +feature
(A) Wire ML prediction into generation pipeline in src/services/hypothetical_service.py: when method=ml_assisted, use classifier to validate/suggest topics, use regressor to pre-score and reject low-quality outputs, use clusterer to ensure diversity against recent generations +feature
(A) Wire ML prediction into validation_service.py: if trained classifier exists, add ml_topic_check (compare LLM-generated topics vs ML-predicted topics, flag mismatches); if trained regressor exists, add ml_quality_check (predict quality score, compare with deterministic score) +feature
(B) Fix SQL injection vulnerability in src/services/database_service.py: replace string interpolation in search_by_topics method (around line 251) with parameterized query using ? placeholders +security
(B) Cap _generation_history list in src/services/hypothetical_service.py to max 100 entries, evict oldest when full; add configurable max_history_size to settings +stability
(B) Add configurable timeout to all LLM generate calls in src/services/llm_service.py: default 120s for generation, 30s for health checks; raise TimeoutError with descriptive message +stability
(B) Move hardcoded corpus path (corpus/clean/tort/corpus.json) in src/services/corpus_service.py to settings.corpus_path; move hardcoded DB path (data/jikai.db) in src/services/database_service.py to settings.database_path; move embedding model name in src/services/vector_service.py to settings +refactor
(B) Add API endpoints for new features in src/api/main.py: POST /ml/train (trigger training), GET /ml/status (model status+metrics), GET /providers (list providers+models), PUT /providers/default (set default), GET /generate with provider+model query params +feature
(B) Create src/tui/widgets/loading.py: reusable loading spinner widget, progress bar widget with percentage+ETA, status checklist widget (green check / red x / yellow spinner per item) for use across all screens +feature
(B) Create src/tui/widgets/topic_selector.py: multi-select checkbox tree grouped by tort category (General Negligence, Intentional Torts, Land Torts, Strict Liability, Defences); select-all per category; search/filter; show count of selected +feature
(B) Update Makefile: remove docker targets (docker-build, docker-up, docker-down), add `tui` target (python -m src.tui), add `api` target (uvicorn), add `train` target (python -m src.ml.pipeline --data corpus/labelled.csv), add `test` target (pytest), add `setup` target (pip install -r requirements.txt) +infra
(B) Add tests for AnthropicProvider, GoogleGeminiProvider, LocalLLMProvider in tests/test_services/test_llm_providers.py: mock HTTP responses, test retry logic, test error handling, test model listing +feature
(B) Add tests for ML pipeline in tests/test_ml/: test_classifier.py (train on small fixture, predict, evaluate metrics), test_regressor.py (train, predict, evaluate), test_clustering.py (fit, predict_cluster), test_pipeline.py (train_all, predict) +feature
(B) Add snapshot tests for TUI screens in tests/test_tui/: test each screen renders correctly with mock data, test navigation between screens, test generate flow with mock LLM response +feature
(B) Create sample labelled training data at corpus/labelled/sample.csv with 20 entries: columns text,topic_labels,quality_score,difficulty_level; topic_labels as pipe-separated string; quality_score 1.0-10.0; difficulty_level easy/medium/hard +feature
(B) Add generation method selection to GenerationRequest model in src/services/hypothetical_service.py: add field method: Literal["pure_llm", "ml_assisted", "hybrid"] with default "pure_llm"; add field provider: Optional[str] and model: Optional[str] for per-request LLM selection +feature
(B) Implement hybrid generation mode in src/services/hypothetical_service.py: use ML classifier to suggest/validate topics, generate with LLM, use ML regressor to score output, if score < threshold regenerate with adjusted prompt, use clusterer to check diversity against last 10 generations +feature
(B) Remove all Docker references from README.md: remove Docker setup instructions, add local setup instructions (python -m venv, pip install, make setup, make tui), add section on API key configuration, add section on ML training +infra
(B) Add error boundary and circuit breaker pattern to LLM provider calls: after 3 consecutive failures on a provider, mark it as unhealthy for 60s, auto-fallback to next available provider, show status in TUI provider screen +stability
(B) Create src/services/llm_providers/__init__.py that auto-discovers and registers all provider classes from the package, exports ProviderRegistry singleton +refactor
(B) Add cost tracking to LLM providers: estimate token costs per provider/model (configurable price table in settings), accumulate per-session cost, display in TUI footer and generation results +feature
(B) Add corpus import/export in TUI corpus screen: import from CSV/JSON file, export selected entries to CSV/JSON, validate schema on import +feature
(B) Update src/services/prompt_engineering/templates.py: make templates aware of expanded subtopics, add topic-specific prompt hints for new subtopics (e.g. occupiers_liability -> include premises description, visitor classification) +feature
(B) Implement async streaming in TUI generate screen: if provider supports streaming, show tokens appearing in real-time in output panel; fallback to loading spinner + full response for non-streaming providers +feature
