Recommended pipeline: 

1. Collect and sanitise dataset (pandas, nltk, spacy, re)
2. Select pretrained models to finetune (transformers, torch) OR train your own model from scratch (pytorch, keras)
3. Prepare dataset for training (datasets, transformers)
4. Finetune the model (transformers, torch)
5. Generate hypotheticals (transformers, torch)
6. Evaluate and refine as needed

(A) Collate available Tort law, Contract 1, Contract 2 and Crim law hypo datasets and ask GPT what format I should present them in and how the workflow should work, use Pytorch or Keras or some other commonly used library as needed @core @basis
(A) Implement OCR from PDFs to handle rendering the corpus to begin with @core @basis
(A) Ask GPT to suggest a learning plan and follow along with how to train my own model from scratch in prep for the intro-to-ai mod next sem, reference this channel (https://youtube.com/@statquest?si=TsXa6S6xCMznv0nH) as and where its relevant @core @basis
(A) Take this opportunity to learn how to train a LLM model on specific hypo datasets from scratch, only using specified low-level core dependancies like langchain or those suited for law instead of using pretrained models @core @basis

(A) Allow for students to generate practise hypos based on specified parameters @core @ideas @feature
(A) Collate core subjects I want Jikai to support (tort law, criminal law, contract law 1, contract law 2, ip law) @core @ideas
(A) Brainstorm further user functionality that allows them to comment on hypos, rate hypos etc. @core @ideas
(A) Consider implementing more than one LLM so users can rate and jointly decide which LLMs are better at generating hypos @core @ideas
(A) Consider using https://github.com/explosion/spaCy to extract relevant information from the hypo for users, OR to implement an agentic workflow @core @ideas
(A) Implementing agentic workflows to test and randomly sample the generated hypos and issues being spotted to validate the quality of each hypo @core @testing
(A) Consider implementing a textbox or textarea for users to actually type their responses in so they can practise on the website itself @core @testing

(B) Look into launching the webapp with Jinja (https://jinja.palletsprojects.com/en/stable/) or another convenient to work with Python Web framework @web @feature
(B) Figure out what technologies I can use to migrate jikai to a full-stack webapp @web @feature
(B) Consider using flask or bottle (https://github.com/bottlepy/bottle) to handle the database interaction and backend routing logic @web @feature @backend

(C) Allow for students to log in with an account and store their respective hypos and cases on this account @web @feature @user

(D) Better code and all documentation to prepare jikai for release @admin 

(E) Backup plan: Always prioritise training my own models for hypo generation first, but if there's a limit to how well that goes then consider relying on OpenAI's API pricing per here (https://openai.com/api/pricing/) and see whether I can integrate Chatlab (https://github.com/rgbkrk/chatlab) so OpenAI can call my own predefined functions @ideas @future
(E) Backup plan: Work out what LLMs (local/remote) I can use via api calls or direct prompting to generate reliable hypos and their respective issues and tags @core @function

(E) Additional: Look into bertopic (https://github.com/MaartenGr/BERTopic) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(E) Additional: Look into f5-tts (https://github.com/SWivid/F5-TTS) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(E) Additional: Look into bm25s (https://github.com/xhluca/bm25s) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(E) Additional: Look into LangChain (https://langchain.com/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(E) Additional: Look into Hugging Face (https://huggingface.co/) and their pretrained models (GPT-J, GPT-Neo, LLaMA) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(E) Additional: Look into Haystack (https://haystack.deepset.ai/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(E) Additional: Look into Fine-tuned LLaMA variants for conversational AI like Vicuna (https://lmsys.org/blog/2023-03-30-vicuna/) and Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(E) Additional: Look into FOSS models like Falcon (https://falconllm.tii.ae/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(E) Additional: Look into Lightweight models for resource efficiency like Mistral (https://mistral.ai/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future