x (A) Collate available Tort law, Contract 1, Contract 2 and Crim law hypo datasets and ask GPT what format I should present them in and how the workflow should work, use Pytorch or Keras or some other commonly used library as needed @core @basis
x (A) Add hardcoded function to helper.py that scans hypos within the corpus for the relevant subject tags first against the subject tags specified by the user, then trains and queries the local model based on those only. @basis @core
x (A) Consider implementing the above idea but maintaining the langchain vector store for the hypos cherry-picked by topic for prompting so that the actual hypos fed can still be recalled by the pretrained internal ollama model's context @core @basis
x (A) Add a function that chooses how many of the relevant_text_data to provide, then filters them by importance or integrate this into the query_relevant_text() function @core @basis
x (A) For debugging purposes, integrate a logging function that logs all parameters and outputs to local JSON files similar to what I did for Elefant's data validaiton pipeline @core @basis
x (A) For debugging's sake, add a logging function that logs these respective agent's outputs and judgements as well @core @basis
x (A) Add a mermaid diagram that explains this workflow in the README.md @core @basis
x (A) Implement another Ollama agent that then reviews the generated hypo and determines whether the specified topics are present, if even one is missing then regenerate hypo as part of an agentic workflow @core @basis
x (A) Implement a second agentic check where another agent checks how similar the example hypo(s) and the generated hypo are to each other @core @basis
x (A) Implement a third "test agent" that is then meant to read the generated hypo and answer a set of predetermined questions like number of parties, issue spotting etc and churn out possible topic sentencs @core @basis 

(A) For documentation's sake, note down all possible tort law topics that can be used to prompt the model to begin with, might have to go into the corpus to see @core @basis
(A) Now add additional logic to parse the response of agent1 and agent2, and conditional flows where if either one check fails then flagged but if both failed the hypo is regenerated @core @basis
(A) Consider finetuning the prompt for the legal analysis agent to ask for it to also deliver its response in a structured fashion like IRAC and to deliver specific topic sentences @core @basis
(A) Check whether the query_relevant_text() function in helper.py is working as expected when sample_size is larger than 1 @core @basis
(A) Add a summary function called execute_XXX_workflow() similar to the one in elegant_elefant which handles everything to bundle all execution code within main.py @core @basis
(A) Continue tweaking the prompts within query_model() function in helper.py to make them as specific as necessary @core @basis

(A) Determine if there are other possible variables I want to tweak in the prompt I generate for query_model, like number of parties, releavant issues, domain of law (contract, criminal, tort), etc, ask GPT for help with what other possible prompts there are @core @basis
(A) Consider using a venv to run everything instead of downloading it to my system root @core @basis
(A) Also clean up the requirements.txt file to ensure unneeded dependancies are not being installed @core @basis

(A) Also consider what other available ollama models there are that are better at creative and coherent text/scenario/hypotheticals generation, ask GPT for help with this as well @core @basis

(A) Train these local Ollama models with long-term context by using tools like MEMORYBANK or HAYSTACK so memory can be retained based on the hypo data across sessions @core @basis
(A) Look into other alternatives that allow local LLM models to retain context and memory across query sessions, ask GPT and perplexity ai for help @core @basis
(A) Build Jikai's base of generating hypos based on specified topics with pretrained local LLM models like ollama client first @core @basis
(A) Look into Ollama in particular for excellent CLI support ++ Hugging Face (https://huggingface.co/) and their pretrained models (GPT-J, GPT-Neo, LLaMA) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Look into what available models there are similar to Ollama's local clients that can read existing text and 'learn' them to then produce different specified output @core @basis
(A) Debug the local dockerfile especially in relation to the local filepath for execution of the ./src/main.py and README workflow for using that dockerfile @core @deployment

(A) Additional: Look into bertopic (https://github.com/MaartenGr/BERTopic) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(A) Additional: Look into f5-tts (https://github.com/SWivid/F5-TTS) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(A) Additional: Look into bm25s (https://github.com/xhluca/bm25s) and see whether there's a way to integrate it in a value-adding manner to jikai @ideas @future
(A) Additional: Look into Haystack (https://haystack.deepset.ai/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Additional: Look into Fine-tuned LLaMA variants for conversational AI like Vicuna (https://lmsys.org/blog/2023-03-30-vicuna/) and Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Additional: Look into FOSS models like Falcon (https://falconllm.tii.ae/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Additional: Look into Lightweight models for resource efficiency like Mistral (https://mistral.ai/) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Additional: Look into langgraph (https://www.langchain.com/langgraph) and see whether there's a way to integrate it in in a value-adding manner to jikai @ideas @future
(A) Additional: Implement OCR from PDFs to handle rendering the corpus for future documents that require more detailed preprocessing @ideas @future

(B) Allow for students to generate practise hypos based on specified parameters @core @ideas @feature
(B) Collate core subjects I want Jikai to support (tort law, criminal law, contract law 1, contract law 2, ip law) @core @ideas
(B) Brainstorm further user functionality that allows them to comment on hypos, rate hypos etc. @core @ideas
(B) Consider implementing more than one LLM so users can rate and jointly decide which LLMs are better at generating hypos @core @ideas
(B) Consider using https://github.com/explosion/spaCy to extract relevant information from the hypo for users, OR to implement an agentic workflow @core @ideas
(B) Implementing agentic workflows to test and randomly sample the generated hypos and issues being spotted to validate the quality of each hypo @core @testing
(B) Consider implementing a textbox or textarea for users to actually type their responses in so they can practise on the website itself @core @testing

(B) Add a disclaimer to the README.md excusing myself of any and all liability particularly relating to defamation under libel, ask GPT for further legal advice as required @legal @admin
(B) Look into launching the webapp with Jinja (https://jinja.palletsprojects.com/en/stable/) or another convenient to work with Python Web framework @web @feature
(B) Figure out what technologies I can use to migrate jikai to a full-stack webapp @web @feature
(B) Consider using flask or bottle (https://github.com/bottlepy/bottle) to handle the database interaction and backend routing logic @web @feature @backend

(C) Allow for students to log in with an account and store their respective hypos and cases on this account @web @feature @user

(D) Better code and all documentation to prepare jikai for release @admin 

(E) Backup plan: Always prioritise training my own models for hypo generation first, but if there's a limit to how well that goes then consider relying on OpenAI's API pricing per here (https://openai.com/api/pricing/) and see whether I can integrate Chatlab (https://github.com/rgbkrk/chatlab) so OpenAI can call my own predefined functions @ideas @future
(E) Backup plan: Work out what LLMs (local/remote) I can use via api calls or direct prompting to generate reliable hypos and their respective issues and tags @core @function

(E) Consider implementing unified interface for LLM inquiry with langchain within my implementation of internal agents within the agentic workflow validation 
(E) Look into using other locally trained agents for agentic workflows, such as Hugging Face's pretrained models (GPT-J, GPT-Neo, LLaMA), Fine-tuned LLaMA variants for conversational AI (Vicuna, Alpaca), FOSS models (Falcon) and Microsoft Deepspeed 
(E) See if there's any space to integrate doc2vec technology into the data_pipeline_validation workflow similar to https://github.com/gongahkia/the-archives/tree/main/python/learning_doc2vec 

(F) Focus on training the model on hypotheticals within the domain of tort law first @core @basis
(F) Debug the breaking code within ./all.py and actually understand and annotate what each line is doing @core @basis
(F) Train the model on the sanitised corpus within clean/tort/corpus.json @core @basis
(F) Work on training a LLM model from scratch and work on code within ./src/scratch @core @basis
(F) Ask GPT to suggest a learning plan and follow along with how to train my own model from scratch in prep for the intro-to-ai mod next sem, reference this channel (https://youtube.com/@statquest?si=TsXa6S6xCMznv0nH) as and where its relevant @core @basis
(F) Add mermaid diagrams of the complete workflow for training models completely from scratch to the README.md so the user has the option to use either route and just for better documentation @core @basis @admin
(F) Take this opportunity to learn how to train and finetune a LLM model on specific hypo datasets from scratch, only using specified low-level core dependancies like langchain or those suited for law instead of using pretrained models @core @basis
