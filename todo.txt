(B) Add tests for AnthropicProvider, GoogleGeminiProvider, LocalLLMProvider in tests/test_services/test_llm_providers.py: mock HTTP responses, test retry logic, test error handling, test model listing +feature
(B) Add tests for ML pipeline in tests/test_ml/: test_classifier.py (train on small fixture, predict, evaluate metrics), test_regressor.py (train, predict, evaluate), test_clustering.py (fit, predict_cluster), test_pipeline.py (train_all, predict) +feature
(B) Add snapshot tests for TUI screens in tests/test_tui/: test each screen renders correctly with mock data, test navigation between screens, test generate flow with mock LLM response +feature
(B) Create sample labelled training data at corpus/labelled/sample.csv with 20 entries: columns text,topic_labels,quality_score,difficulty_level; topic_labels as pipe-separated string; quality_score 1.0-10.0; difficulty_level easy/medium/hard +feature
(B) Add generation method selection to GenerationRequest model in src/services/hypothetical_service.py: add field method: Literal["pure_llm", "ml_assisted", "hybrid"] with default "pure_llm"; add field provider: Optional[str] and model: Optional[str] for per-request LLM selection +feature
(B) Implement hybrid generation mode in src/services/hypothetical_service.py: use ML classifier to suggest/validate topics, generate with LLM, use ML regressor to score output, if score < threshold regenerate with adjusted prompt, use clusterer to check diversity against last 10 generations +feature
(B) Remove all Docker references from README.md: remove Docker setup instructions, add local setup instructions (python -m venv, pip install, make setup, make tui), add section on API key configuration, add section on ML training +infra
(B) Add error boundary and circuit breaker pattern to LLM provider calls: after 3 consecutive failures on a provider, mark it as unhealthy for 60s, auto-fallback to next available provider, show status in TUI provider screen +stability
(B) Create src/services/llm_providers/__init__.py that auto-discovers and registers all provider classes from the package, exports ProviderRegistry singleton +refactor
(B) Add cost tracking to LLM providers: estimate token costs per provider/model (configurable price table in settings), accumulate per-session cost, display in TUI footer and generation results +feature
(B) Add corpus import/export in TUI corpus screen: import from CSV/JSON file, export selected entries to CSV/JSON, validate schema on import +feature
(B) Update src/services/prompt_engineering/templates.py: make templates aware of expanded subtopics, add topic-specific prompt hints for new subtopics (e.g. occupiers_liability -> include premises description, visitor classification) +feature
(B) Implement async streaming in TUI generate screen: if provider supports streaming, show tokens appearing in real-time in output panel; fallback to loading spinner + full response for non-streaming providers +feature
