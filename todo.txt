(B) Add cost tracking to LLM providers: estimate token costs per provider/model (configurable price table in settings), accumulate per-session cost, display in TUI footer and generation results +feature
(B) Add corpus import/export in TUI corpus screen: import from CSV/JSON file, export selected entries to CSV/JSON, validate schema on import +feature
(B) Update src/services/prompt_engineering/templates.py: make templates aware of expanded subtopics, add topic-specific prompt hints for new subtopics (e.g. occupiers_liability -> include premises description, visitor classification) +feature
(B) Implement async streaming in TUI generate screen: if provider supports streaming, show tokens appearing in real-time in output panel; fallback to loading spinner + full response for non-streaming providers +feature
