# ----- required imports -----

import json

# ----- helper functions -----


def nanoseconds_to_seconds(nanoseconds):
    """
    convert nanoseconds to seconds
    """
    return nanoseconds / 1_000_000_000.0


def load_corpus(filepath):
    """
    reads the local corpus json file and
    returns the data
    """
    try:
        with open(filepath, "r") as file:
            return json.load(file)
    except Exception as e:
        print(f"Error loading corpus: {e}")
        return None


def query_relevant_text(corpus, topics, sample_size):
    """
    retrieves relevant texts from the vector store based on topic,
    prioritizing those with the most overlapping topics
    """
    relevant_texts = []
    for entry in corpus:
        entry_topics = entry["topic"]
        overlap_count = len(set(entry_topics) & set(topics))
        if overlap_count > 0:
            relevant_texts.append((entry["text"], overlap_count))
    relevant_texts.sort(key=lambda x: x[1], reverse=True)
    # print(f"relevant texts sorted are: {relevant_texts}")
    prioritized_texts = [text for text, _ in relevant_texts[:sample_size]]
    return prioritized_texts


def sanitise_data(raw_response):
    """
    accepts ollama client model's complete output and groups relevant data into a json
    """
    return {
        "model": {
            "model_name": raw_response["model"],
            "model_creation_time": raw_response["created_at"],
        },
        "duration": {
            "load_model_duration": nanoseconds_to_seconds(
                raw_response["load_duration"]
            ),
            "prompt_evaluation_duration": nanoseconds_to_seconds(
                raw_response["prompt_eval_duration"]
            ),
            "response_generation_duration": nanoseconds_to_seconds(
                raw_response["eval_duration"]
            ),
            "total_duration": nanoseconds_to_seconds(raw_response["total_duration"]),
        },
        "tokens": {
            "response_tokens_count": raw_response["eval_count"],
            "response_tokens_per_second": raw_response["eval_count"]
            / nanoseconds_to_seconds(raw_response["eval_duration"]),
        },
        "response": raw_response["response"],
    }


def write_agent_log(
    target_filepath, field_value, field_name="agent_log", entry_num="1"
):
    """
    writes log data generated by the agentic workflow to the
    json at the specified filepath
    """
    try:
        with open(target_filepath, "r") as json_file:
            data = json.load(json_file)[entry_num]
        wrapper = {
            entry_num: {
                "sift_1_validation_status": data["sift_1_validation_status"],
                "sift_2_validation_status": data["sift_2_validation_status"],
                "clean_html_body_local_file_path": data[
                    "clean_html_body_local_file_path"
                ],
                "sift_1_error_array": data["sift_1_error_array"],
                "sift_2_error_array": data["sift_2_error_array"],
                field_name: field_value,
                "enriched_data_entry_json": data["enriched_data_entry_json"],
            }
        }
        with open(target_filepath, "w") as json_file:
            json.dump(wrapper, json_file, indent=4)
        print(
            f"Data successfully written to JSON at the filepath {target_filepath}\nField '{field_name}' successfully appended."
        )
    except Exception as e:
        print(
            f"Error: Unable to read or write to the JSON at the specified filepath: {e}"
        )
